1:"$Sreact.fragment"
2:I[52669,["/_next/static/chunks/64f978679dabc1f7.js","/_next/static/chunks/90402e289e15bc89.js","/_next/static/chunks/366210a841ce1f0c.js"],"default"]
3:I[17472,["/_next/static/chunks/64f978679dabc1f7.js","/_next/static/chunks/90402e289e15bc89.js","/_next/static/chunks/366210a841ce1f0c.js"],"ThemeStoreProvider"]
4:I[17350,["/_next/static/chunks/64f978679dabc1f7.js","/_next/static/chunks/90402e289e15bc89.js","/_next/static/chunks/366210a841ce1f0c.js"],"default"]
5:I[79606,["/_next/static/chunks/64f978679dabc1f7.js","/_next/static/chunks/90402e289e15bc89.js","/_next/static/chunks/366210a841ce1f0c.js"],"default"]
6:I[19646,["/_next/static/chunks/64f978679dabc1f7.js","/_next/static/chunks/90402e289e15bc89.js","/_next/static/chunks/366210a841ce1f0c.js","/_next/static/chunks/d37bf3c64d792514.js","/_next/static/chunks/ba2ff735ad6cf3a9.js","/_next/static/chunks/dab6b902f1f46a02.js","/_next/static/chunks/fdc3a39a7eee7a91.js"],"default"]
7:I[77324,["/_next/static/chunks/64f978679dabc1f7.js","/_next/static/chunks/90402e289e15bc89.js","/_next/static/chunks/366210a841ce1f0c.js"],"default"]
8:I[39756,["/_next/static/chunks/ff1a16fafef87110.js","/_next/static/chunks/d2be314c3ece3fbe.js"],"default"]
9:I[37457,["/_next/static/chunks/ff1a16fafef87110.js","/_next/static/chunks/d2be314c3ece3fbe.js"],"default"]
b:I[97367,["/_next/static/chunks/ff1a16fafef87110.js","/_next/static/chunks/d2be314c3ece3fbe.js"],"OutletBoundary"]
c:"$Sreact.suspense"
e:I[97367,["/_next/static/chunks/ff1a16fafef87110.js","/_next/static/chunks/d2be314c3ece3fbe.js"],"ViewportBoundary"]
10:I[97367,["/_next/static/chunks/ff1a16fafef87110.js","/_next/static/chunks/d2be314c3ece3fbe.js"],"MetadataBoundary"]
12:I[68027,[],"default"]
:HL["/_next/static/chunks/7b37af83fbd0b6f8.css","style"]
:HL["/_next/static/media/797e433ab948586e-s.p.dbea232f.woff2","font",{"crossOrigin":"","type":"font/woff2"}]
:HL["/_next/static/media/caa3a2e1cccd8315-s.p.853070df.woff2","font",{"crossOrigin":"","type":"font/woff2"}]
:HL["/_next/static/media/ce62453a442c7f35-s.p.a9507876.woff2","font",{"crossOrigin":"","type":"font/woff2"}]
:HL["https://fonts.googleapis.com/css2?family=Material+Symbols+Outlined:opsz,wght,FILL,GRAD@20..48,100..700,0..1,-50..200&icon_names=domino_mask","style"]
0:{"P":null,"b":"SE7nSNVOxToI_LcdvxZLU","c":["","posts","deep-learning","crypto-llm"],"q":"","i":false,"f":[[["",{"children":["posts",{"children":[["slug","deep-learning/crypto-llm","c"],{"children":["__PAGE__",{}]}]}]},"$undefined","$undefined",true],[["$","$1","c",{"children":[[["$","link","0",{"rel":"stylesheet","href":"/_next/static/chunks/7b37af83fbd0b6f8.css","precedence":"next","crossOrigin":"$undefined","nonce":"$undefined"}],["$","script","script-0",{"src":"/_next/static/chunks/64f978679dabc1f7.js","async":true,"nonce":"$undefined"}],["$","script","script-1",{"src":"/_next/static/chunks/90402e289e15bc89.js","async":true,"nonce":"$undefined"}],["$","script","script-2",{"src":"/_next/static/chunks/366210a841ce1f0c.js","async":true,"nonce":"$undefined"}]],["$","html",null,{"lang":"en","className":"roboto_d24be3b7-module__pZO6tq__variable dark min-h-screen","suppressHydrationWarning":true,"children":[["$","head",null,{"children":["$","link",null,{"rel":"stylesheet","href":"https://fonts.googleapis.com/css2?family=Material+Symbols+Outlined:opsz,wght,FILL,GRAD@20..48,100..700,0..1,-50..200&icon_names=domino_mask"}]}],["$","body",null,{"className":"geist_a71539c9-module__T19VSG__variable geist_mono_8d43a2aa-module__8Li5zG__variable min-h-screen antialiased","suppressHydrationWarning":true,"style":{"paddingBottom":"100px"},"children":["$","$L2",null,{"options":{"enableCssLayer":true},"children":["$","$L3",null,{"children":["$","$L4",null,{"theme":"$5","disableTransitionOnChange":true,"noSsr":true,"defaultMode":"dark","children":["$","$L6",null,{"className":"min-h-screen w-full","sx":{"bgcolor":"background.default"},"children":[["$","$L7",null,{}],["$","$L8",null,{"parallelRouterKey":"children","error":"$undefined","errorStyles":"$undefined","errorScripts":"$undefined","template":["$","$L9",null,{}],"templateStyles":"$undefined","templateScripts":"$undefined","notFound":[[["$","title",null,{"children":"404: This page could not be found."}],["$","div",null,{"style":{"fontFamily":"system-ui,\"Segoe UI\",Roboto,Helvetica,Arial,sans-serif,\"Apple Color Emoji\",\"Segoe UI Emoji\"","height":"100vh","textAlign":"center","display":"flex","flexDirection":"column","alignItems":"center","justifyContent":"center"},"children":["$","div",null,{"children":[["$","style",null,{"dangerouslySetInnerHTML":{"__html":"body{color:#000;background:#fff;margin:0}.next-error-h1{border-right:1px solid rgba(0,0,0,.3)}@media (prefers-color-scheme:dark){body{color:#fff;background:#000}.next-error-h1{border-right:1px solid rgba(255,255,255,.3)}}"}}],["$","h1",null,{"className":"next-error-h1","style":{"display":"inline-block","margin":"0 20px 0 0","padding":"0 23px 0 0","fontSize":24,"fontWeight":500,"verticalAlign":"top","lineHeight":"49px"},"children":404}],["$","div",null,{"style":{"display":"inline-block"},"children":["$","h2",null,{"style":{"fontSize":14,"fontWeight":400,"lineHeight":"49px","margin":0},"children":"This page could not be found."}]}]]}]}]],[]],"forbidden":"$undefined","unauthorized":"$undefined"}]]}]}]}]}]}]]}]]}],{"children":[["$","$1","c",{"children":[null,["$","$L8",null,{"parallelRouterKey":"children","error":"$undefined","errorStyles":"$undefined","errorScripts":"$undefined","template":["$","$L9",null,{}],"templateStyles":"$undefined","templateScripts":"$undefined","notFound":"$undefined","forbidden":"$undefined","unauthorized":"$undefined"}]]}],{"children":[["$","$1","c",{"children":[null,["$","$L8",null,{"parallelRouterKey":"children","error":"$undefined","errorStyles":"$undefined","errorScripts":"$undefined","template":["$","$L9",null,{}],"templateStyles":"$undefined","templateScripts":"$undefined","notFound":"$undefined","forbidden":"$undefined","unauthorized":"$undefined"}]]}],{"children":[["$","$1","c",{"children":["$La",[["$","script","script-0",{"src":"/_next/static/chunks/d37bf3c64d792514.js","async":true,"nonce":"$undefined"}],["$","script","script-1",{"src":"/_next/static/chunks/ba2ff735ad6cf3a9.js","async":true,"nonce":"$undefined"}],["$","script","script-2",{"src":"/_next/static/chunks/dab6b902f1f46a02.js","async":true,"nonce":"$undefined"}],["$","script","script-3",{"src":"/_next/static/chunks/fdc3a39a7eee7a91.js","async":true,"nonce":"$undefined"}]],["$","$Lb",null,{"children":["$","$c",null,{"name":"Next.MetadataOutlet","children":"$@d"}]}]]}],{},null,false,false]},null,false,false]},null,false,false]},null,false,false],["$","$1","h",{"children":[null,["$","$Le",null,{"children":"$Lf"}],["$","div",null,{"hidden":true,"children":["$","$L10",null,{"children":["$","$c",null,{"name":"Next.Metadata","children":"$L11"}]}]}],["$","meta",null,{"name":"next-size-adjust","content":""}]]}],false]],"m":"$undefined","G":["$12",[]],"S":true}
13:I[52057,["/_next/static/chunks/64f978679dabc1f7.js","/_next/static/chunks/90402e289e15bc89.js","/_next/static/chunks/366210a841ce1f0c.js","/_next/static/chunks/d37bf3c64d792514.js","/_next/static/chunks/ba2ff735ad6cf3a9.js","/_next/static/chunks/dab6b902f1f46a02.js","/_next/static/chunks/fdc3a39a7eee7a91.js"],"default"]
14:I[5500,["/_next/static/chunks/64f978679dabc1f7.js","/_next/static/chunks/90402e289e15bc89.js","/_next/static/chunks/366210a841ce1f0c.js","/_next/static/chunks/d37bf3c64d792514.js","/_next/static/chunks/ba2ff735ad6cf3a9.js","/_next/static/chunks/dab6b902f1f46a02.js","/_next/static/chunks/fdc3a39a7eee7a91.js"],"Image"]
15:I[26863,["/_next/static/chunks/64f978679dabc1f7.js","/_next/static/chunks/90402e289e15bc89.js","/_next/static/chunks/366210a841ce1f0c.js","/_next/static/chunks/d37bf3c64d792514.js","/_next/static/chunks/ba2ff735ad6cf3a9.js","/_next/static/chunks/dab6b902f1f46a02.js","/_next/static/chunks/fdc3a39a7eee7a91.js"],"default"]
16:I[614,["/_next/static/chunks/64f978679dabc1f7.js","/_next/static/chunks/90402e289e15bc89.js","/_next/static/chunks/366210a841ce1f0c.js","/_next/static/chunks/d37bf3c64d792514.js","/_next/static/chunks/ba2ff735ad6cf3a9.js","/_next/static/chunks/dab6b902f1f46a02.js","/_next/static/chunks/fdc3a39a7eee7a91.js"],"default"]
17:I[36773,["/_next/static/chunks/64f978679dabc1f7.js","/_next/static/chunks/90402e289e15bc89.js","/_next/static/chunks/366210a841ce1f0c.js","/_next/static/chunks/d37bf3c64d792514.js","/_next/static/chunks/ba2ff735ad6cf3a9.js","/_next/static/chunks/dab6b902f1f46a02.js","/_next/static/chunks/fdc3a39a7eee7a91.js"],"default"]
a:["$","$L13",null,{"children":[["$","$L6",null,{"position":"relative","width":{"xs":"100vw","sm":"100%"},"marginBottom":2,"marginX":{"xs":-2,"sm":0},"height":{"xs":300,"sm":600},"children":[["$","$L14",null,{"src":"/articles/crypto-llm/cover.png","alt":"An image showing encrypted & decrypted text","fill":true,"style":{"objectFit":"cover"},"sizes":"(max-width: 600px) 384px, 1080px","quality":90}],null]}],["$","$L6",null,{"children":[["$","$L15",null,{"variant":"h2","sx":{"fontWeight":"bold","fontSize":{"xs":"250%","sm":"400%"}},"children":"A method for text to text encryption using LLMs"}],["$","$L16",null,{"direction":"row","spacing":4,"children":[["$","$L15",null,{"id":"publication-date","variant":"overline","children":"1 February 2026"}],["$","$L15",null,{"id":"tags","variant":"overline","children":"deep learning | math | llm | crypto"}]]}],["$","$L17",null,{}]]}],["$","$L6",null,{"children":["$","article",null,{"className":"prose dark:prose-invert","children":[[["$","p",null,{"children":["In this article, I present a novel (and probably useless but fun) use of language models: ",["$","strong",null,{"children":"encrypting text as text"}],"."]}],"\n",["$","h1",null,{"children":"Language models can be used to compress text"}],"\n",["$","p",null,{"children":["Given a sequence of tokens, language models predict the probability distribution of the next token (if you’re unfamiliar with tokens, you can roughly think of them as words or word fragments). Because they define a conditional probability model over text, they can be used as the probabilistic component of an arithmetic coder to compress text (see, for example, ",["$","a",null,{"href":"https://github.com/AlexBuz/llama-zip","children":"llama-zip"}],"). An arithmetic coder is a compression algorithm that leverages a statistical model to make probable data take up less space than rare data."]}],"\n",["$","p",null,{"children":"When compression is effective, text is transformed into a seemingly unpredictable sequence of bits. Unpredictability is a hallmark of near-optimal compression. The arithmetic decoder then reconstructs the original text exactly. In practice, however, this application is mostly useless: the size of the model parameters typically dwarfs the size of any text worth compressing."}],"\n",["$","h1",null,{"children":"Language models can be used to encode data as text"}],"\n",["$","p",null,{"children":"This raises a more interesting question: can we do the opposite? That is, can we turn an arbitrary sequence of bits into text and later recover the original bits? If so, we could transmit any message that can be expressed as bits in the form of a plausible-looking text. Naturally, this is even less useful as it is simply a prohibitively expensive way to store binary data."}],"\n",["$","p",null,{"children":"The main difficulty is that while arithmetic coding guarantees that:"}],"\n",["$","span",null,{"className":"katex-display","children":["$","span",null,{"className":"katex","children":[["$","span",null,{"className":"katex-mathml","children":["$","math",null,{"xmlns":"http://www.w3.org/1998/Math/MathML","display":"block","children":["$","semantics",null,{"children":[["$","mrow",null,{"children":[["$","mtext",null,{"children":"text"}],["$","mo",null,{"children":"→"}],["$","mtext",null,{"children":"bits"}],["$","mo",null,{"children":"→"}],["$","mtext",null,{"children":"text"}]]}],["$","annotation",null,{"encoding":"application/x-tex","children":"\\text{text} \\rightarrow \\text{bits} \\rightarrow \\text{text}"}]]}]}]}],["$","span",null,{"className":"katex-html","aria-hidden":"true","children":[["$","span",null,{"className":"base","children":[["$","span",null,{"className":"strut","style":{"height":"0.6151em"}}],["$","span",null,{"className":"mord text","children":["$","span",null,{"className":"mord","children":"text"}]}],["$","span",null,{"className":"mspace","style":{"marginRight":"0.2778em"}}],["$","span",null,{"className":"mrel","children":"→"}],["$","span",null,{"className":"mspace","style":{"marginRight":"0.2778em"}}]]}],["$","span",null,{"className":"base","children":[["$","span",null,{"className":"strut","style":{"height":"0.6944em"}}],["$","span",null,{"className":"mord text","children":["$","span",null,{"className":"mord","children":"bits"}]}],"$L18","$L19","$L1a"]}],"$L1b"]}]]}]}],"\n","$L1c","\n","$L1d","\n","$L1e","\n","$L1f","\n","$L20","\n","$L21","\n","$L22","\n","$L23","\n","$L24","\n","$L25","\n","$L26","\n","$L27","\n","$L28","\n","$L29","\n","$L2a","\n","$L2b","\n","$L2c","\n","$L2d","\n","$L2e","\n","$L2f","\n","$L30","\n","$L31","\n","$L32","\n","$L33","\n","$L34","\n","$L35","\n","$L36","\n","$L37","\n","$L38","\n","$L39","\n","$L3a","\n","$L3b","\n","$L3c","\n","$L3d","\n","$L3e","\n","$L3f"],"$L40",null]}]}]]}]
41:I[92522,["/_next/static/chunks/64f978679dabc1f7.js","/_next/static/chunks/90402e289e15bc89.js","/_next/static/chunks/366210a841ce1f0c.js","/_next/static/chunks/d37bf3c64d792514.js","/_next/static/chunks/ba2ff735ad6cf3a9.js","/_next/static/chunks/dab6b902f1f46a02.js","/_next/static/chunks/fdc3a39a7eee7a91.js"],"default"]
42:I[18594,["/_next/static/chunks/64f978679dabc1f7.js","/_next/static/chunks/90402e289e15bc89.js","/_next/static/chunks/366210a841ce1f0c.js","/_next/static/chunks/d37bf3c64d792514.js","/_next/static/chunks/ba2ff735ad6cf3a9.js","/_next/static/chunks/dab6b902f1f46a02.js","/_next/static/chunks/fdc3a39a7eee7a91.js"],"default"]
43:I[23120,["/_next/static/chunks/64f978679dabc1f7.js","/_next/static/chunks/90402e289e15bc89.js","/_next/static/chunks/366210a841ce1f0c.js","/_next/static/chunks/d37bf3c64d792514.js","/_next/static/chunks/ba2ff735ad6cf3a9.js","/_next/static/chunks/dab6b902f1f46a02.js","/_next/static/chunks/fdc3a39a7eee7a91.js"],"default"]
44:I[83388,["/_next/static/chunks/64f978679dabc1f7.js","/_next/static/chunks/90402e289e15bc89.js","/_next/static/chunks/366210a841ce1f0c.js","/_next/static/chunks/d37bf3c64d792514.js","/_next/static/chunks/ba2ff735ad6cf3a9.js","/_next/static/chunks/dab6b902f1f46a02.js","/_next/static/chunks/fdc3a39a7eee7a91.js"],"default"]
45:I[4685,["/_next/static/chunks/64f978679dabc1f7.js","/_next/static/chunks/90402e289e15bc89.js","/_next/static/chunks/366210a841ce1f0c.js","/_next/static/chunks/d37bf3c64d792514.js","/_next/static/chunks/ba2ff735ad6cf3a9.js","/_next/static/chunks/dab6b902f1f46a02.js","/_next/static/chunks/fdc3a39a7eee7a91.js"],"default"]
46:I[60597,["/_next/static/chunks/64f978679dabc1f7.js","/_next/static/chunks/90402e289e15bc89.js","/_next/static/chunks/366210a841ce1f0c.js","/_next/static/chunks/d37bf3c64d792514.js","/_next/static/chunks/ba2ff735ad6cf3a9.js","/_next/static/chunks/dab6b902f1f46a02.js","/_next/static/chunks/fdc3a39a7eee7a91.js"],"default"]
47:I[52087,["/_next/static/chunks/64f978679dabc1f7.js","/_next/static/chunks/90402e289e15bc89.js","/_next/static/chunks/366210a841ce1f0c.js","/_next/static/chunks/d37bf3c64d792514.js","/_next/static/chunks/ba2ff735ad6cf3a9.js","/_next/static/chunks/dab6b902f1f46a02.js","/_next/static/chunks/fdc3a39a7eee7a91.js"],"default"]
48:I[85301,["/_next/static/chunks/64f978679dabc1f7.js","/_next/static/chunks/90402e289e15bc89.js","/_next/static/chunks/366210a841ce1f0c.js","/_next/static/chunks/d37bf3c64d792514.js","/_next/static/chunks/ba2ff735ad6cf3a9.js","/_next/static/chunks/dab6b902f1f46a02.js","/_next/static/chunks/fdc3a39a7eee7a91.js"],"default"]
49:I[51678,["/_next/static/chunks/64f978679dabc1f7.js","/_next/static/chunks/90402e289e15bc89.js","/_next/static/chunks/366210a841ce1f0c.js","/_next/static/chunks/d37bf3c64d792514.js","/_next/static/chunks/ba2ff735ad6cf3a9.js","/_next/static/chunks/dab6b902f1f46a02.js","/_next/static/chunks/fdc3a39a7eee7a91.js"],"default"]
18:["$","span",null,{"className":"mspace","style":{"marginRight":"0.2778em"}}]
19:["$","span",null,{"className":"mrel","children":"→"}]
1a:["$","span",null,{"className":"mspace","style":{"marginRight":"0.2778em"}}]
1b:["$","span",null,{"className":"base","children":[["$","span",null,{"className":"strut","style":{"height":"0.6151em"}}],["$","span",null,{"className":"mord text","children":["$","span",null,{"className":"mord","children":"text"}]}]]}]
1c:["$","p",null,{"children":["recovers the original text, it does ",["$","em",null,{"children":"not"}]," guarantee that"]}]
1d:["$","span",null,{"className":"katex-display","children":["$","span",null,{"className":"katex","children":[["$","span",null,{"className":"katex-mathml","children":["$","math",null,{"xmlns":"http://www.w3.org/1998/Math/MathML","display":"block","children":["$","semantics",null,{"children":[["$","mrow",null,{"children":[["$","mtext",null,{"children":"bits"}],["$","mo",null,{"children":"→"}],["$","mtext",null,{"children":"text"}],["$","mo",null,{"children":"→"}],["$","mtext",null,{"children":"bits"}]]}],["$","annotation",null,{"encoding":"application/x-tex","children":"\\text{bits} \\rightarrow \\text{text} \\rightarrow \\text{bits}"}]]}]}]}],["$","span",null,{"className":"katex-html","aria-hidden":"true","children":[["$","span",null,{"className":"base","children":[["$","span",null,{"className":"strut","style":{"height":"0.6944em"}}],["$","span",null,{"className":"mord text","children":["$","span",null,{"className":"mord","children":"bits"}]}],["$","span",null,{"className":"mspace","style":{"marginRight":"0.2778em"}}],["$","span",null,{"className":"mrel","children":"→"}],["$","span",null,{"className":"mspace","style":{"marginRight":"0.2778em"}}]]}],["$","span",null,{"className":"base","children":[["$","span",null,{"className":"strut","style":{"height":"0.6151em"}}],["$","span",null,{"className":"mord text","children":["$","span",null,{"className":"mord","children":"text"}]}],["$","span",null,{"className":"mspace","style":{"marginRight":"0.2778em"}}],["$","span",null,{"className":"mrel","children":"→"}],["$","span",null,{"className":"mspace","style":{"marginRight":"0.2778em"}}]]}],["$","span",null,{"className":"base","children":[["$","span",null,{"className":"strut","style":{"height":"0.6944em"}}],["$","span",null,{"className":"mord text","children":["$","span",null,{"className":"mord","children":"bits"}]}]]}]]}]]}]}]
1e:["$","p",null,{"children":"recovers the original bit sequence."}]
1f:["$","p",null,{"children":["That said, we can design a variant of arithmetic coding that effectively reverses the roles of tokens and bits. At each step, the language model predicts a probability distribution over the next token. These probabilities define a partition of the interval ",["$","span",null,{"className":"katex","children":[["$","span",null,{"className":"katex-mathml","children":["$","math",null,{"xmlns":"http://www.w3.org/1998/Math/MathML","children":["$","semantics",null,{"children":[["$","mrow",null,{"children":[["$","mo",null,{"stretchy":"false","children":"["}],["$","mn",null,{"children":"0"}],["$","mo",null,{"separator":"true","children":","}],["$","mn",null,{"children":"1"}],["$","mo",null,{"stretchy":"false","children":"]"}]]}],["$","annotation",null,{"encoding":"application/x-tex","children":"[0,1]"}]]}]}]}],["$","span",null,{"className":"katex-html","aria-hidden":"true","children":["$","span",null,{"className":"base","children":[["$","span",null,{"className":"strut","style":{"height":"1em","verticalAlign":"-0.25em"}}],["$","span",null,{"className":"mopen","children":"["}],["$","span",null,{"className":"mord","children":"0"}],["$","span",null,{"className":"mpunct","children":","}],["$","span",null,{"className":"mspace","style":{"marginRight":"0.1667em"}}],["$","span",null,{"className":"mord","children":"1"}],["$","span",null,{"className":"mclose","children":"]"}]]}]}]]}],", where each token corresponds to a sub-interval whose length is proportional to its predicted probability. In standard arithmetic coding, bits are interpreted as choosing between sub-intervals; here, we instead interpret bits as selecting ",["$","em",null,{"children":"tokens"}]," via this probabilistic partition."]}]
20:["$","p",null,{"children":["While the theory is straightforward, the implementation is not. The token vocabulary is large, and pathological behaviors start to appear when sampling low-probability tokens. In particular, the encoder sometimes produced a sequence of tokens that, once decoded into text and tokenized again, resulted in a ",["$","em",null,{"children":"different"}]," token sequence."]}]
21:["$","p",null,{"children":"This is not a bug in the tokenizer, but a fundamental property: tokenization is not a bijection. It only guarantees"}]
22:["$","span",null,{"className":"katex-display","children":["$","span",null,{"className":"katex","children":[["$","span",null,{"className":"katex-mathml","children":["$","math",null,{"xmlns":"http://www.w3.org/1998/Math/MathML","display":"block","children":["$","semantics",null,{"children":[["$","mrow",null,{"children":[["$","mtext",null,{"children":"text"}],["$","mo",null,{"children":"="}],["$","mtext",null,{"children":"decode"}],["$","mo",null,{"stretchy":"false","children":"("}],["$","mtext",null,{"children":"encode"}],["$","mo",null,{"stretchy":"false","children":"("}],["$","mtext",null,{"children":"text"}],["$","mo",null,{"stretchy":"false","children":")"}],["$","mo",null,{"stretchy":"false","children":")"}]]}],["$","annotation",null,{"encoding":"application/x-tex","children":"\\text{text} = \\text{decode}(\\text{encode}(\\text{text}))"}]]}]}]}],["$","span",null,{"className":"katex-html","aria-hidden":"true","children":[["$","span",null,{"className":"base","children":[["$","span",null,{"className":"strut","style":{"height":"0.6151em"}}],["$","span",null,{"className":"mord text","children":["$","span",null,{"className":"mord","children":"text"}]}],["$","span",null,{"className":"mspace","style":{"marginRight":"0.2778em"}}],["$","span",null,{"className":"mrel","children":"="}],["$","span",null,{"className":"mspace","style":{"marginRight":"0.2778em"}}]]}],["$","span",null,{"className":"base","children":[["$","span",null,{"className":"strut","style":{"height":"1em","verticalAlign":"-0.25em"}}],["$","span",null,{"className":"mord text","children":["$","span",null,{"className":"mord","children":"decode"}]}],["$","span",null,{"className":"mopen","children":"("}],["$","span",null,{"className":"mord text","children":["$","span",null,{"className":"mord","children":"encode"}]}],["$","span",null,{"className":"mopen","children":"("}],["$","span",null,{"className":"mord text","children":["$","span",null,{"className":"mord","children":"text"}]}],["$","span",null,{"className":"mclose","children":"))"}]]}]]}]]}]}]
23:["$","p",null,{"children":"but not"}]
24:["$","span",null,{"className":"katex-display","children":["$","span",null,{"className":"katex","children":[["$","span",null,{"className":"katex-mathml","children":["$","math",null,{"xmlns":"http://www.w3.org/1998/Math/MathML","display":"block","children":["$","semantics",null,{"children":[["$","mrow",null,{"children":[["$","mtext",null,{"children":"tokens"}],["$","mo",null,{"children":"="}],["$","mtext",null,{"children":"encode"}],["$","mo",null,{"stretchy":"false","children":"("}],["$","mtext",null,{"children":"decode"}],["$","mo",null,{"stretchy":"false","children":"("}],["$","mtext",null,{"children":"tokens"}],["$","mo",null,{"stretchy":"false","children":")"}],["$","mo",null,{"stretchy":"false","children":")"}]]}],["$","annotation",null,{"encoding":"application/x-tex","children":"\\text{tokens} = \\text{encode}(\\text{decode}(\\text{tokens}))"}]]}]}]}],["$","span",null,{"className":"katex-html","aria-hidden":"true","children":[["$","span",null,{"className":"base","children":[["$","span",null,{"className":"strut","style":{"height":"0.6944em"}}],["$","span",null,{"className":"mord text","children":["$","span",null,{"className":"mord","children":"tokens"}]}],["$","span",null,{"className":"mspace","style":{"marginRight":"0.2778em"}}],["$","span",null,{"className":"mrel","children":"="}],["$","span",null,{"className":"mspace","style":{"marginRight":"0.2778em"}}]]}],["$","span",null,{"className":"base","children":[["$","span",null,{"className":"strut","style":{"height":"1em","verticalAlign":"-0.25em"}}],["$","span",null,{"className":"mord text","children":["$","span",null,{"className":"mord","children":"encode"}]}],["$","span",null,{"className":"mopen","children":"("}],["$","span",null,{"className":"mord text","children":["$","span",null,{"className":"mord","children":"decode"}]}],["$","span",null,{"className":"mopen","children":"("}],["$","span",null,{"className":"mord text","children":["$","span",null,{"className":"mord","children":"tokens"}]}],["$","span",null,{"className":"mclose","children":"))"}]]}]]}]]}]}]
25:["$","p",null,{"children":"When I first realized this, I nearly gave up. However, there is a crucial observation: the language model is trained to produce token sequences that originate from the tokenizer. As a result, most high-probability tokens satisfy:"}]
26:["$","span",null,{"className":"katex-display","children":["$","span",null,{"className":"katex","children":[["$","span",null,{"className":"katex-mathml","children":["$","math",null,{"xmlns":"http://www.w3.org/1998/Math/MathML","display":"block","children":["$","semantics",null,{"children":[["$","mrow",null,{"children":[["$","mtext",null,{"children":"tokens"}],["$","mo",null,{"children":"="}],["$","mtext",null,{"children":"encode"}],["$","mo",null,{"stretchy":"false","children":"("}],["$","mtext",null,{"children":"decode"}],["$","mo",null,{"stretchy":"false","children":"("}],["$","mtext",null,{"children":"tokens"}],["$","mo",null,{"stretchy":"false","children":")"}],["$","mo",null,{"stretchy":"false","children":")"}]]}],["$","annotation",null,{"encoding":"application/x-tex","children":"\\text{tokens} = \\text{encode}(\\text{decode}(\\text{tokens}))"}]]}]}]}],["$","span",null,{"className":"katex-html","aria-hidden":"true","children":[["$","span",null,{"className":"base","children":[["$","span",null,{"className":"strut","style":{"height":"0.6944em"}}],["$","span",null,{"className":"mord text","children":["$","span",null,{"className":"mord","children":"tokens"}]}],["$","span",null,{"className":"mspace","style":{"marginRight":"0.2778em"}}],["$","span",null,{"className":"mrel","children":"="}],["$","span",null,{"className":"mspace","style":{"marginRight":"0.2778em"}}]]}],["$","span",null,{"className":"base","children":[["$","span",null,{"className":"strut","style":{"height":"1em","verticalAlign":"-0.25em"}}],["$","span",null,{"className":"mord text","children":["$","span",null,{"className":"mord","children":"encode"}]}],["$","span",null,{"className":"mopen","children":"("}],["$","span",null,{"className":"mord text","children":["$","span",null,{"className":"mord","children":"decode"}]}],["$","span",null,{"className":"mopen","children":"("}],["$","span",null,{"className":"mord text","children":["$","span",null,{"className":"mord","children":"tokens"}]}],["$","span",null,{"className":"mclose","children":"))"}]]}]]}]]}]}]
27:["$","p",null,{"children":["To take advantage of this, I restricted sampling to a relatively conservative regime: ",["$","code",null,{"children":"top_k = 200"}],", ",["$","code",null,{"children":"top_p = 0.9"}],", and ",["$","code",null,{"children":"temperature=0.9"}],". This eliminates most problematic tokens and yields well-behaved sequences that survive a decode–re-encode round trip."]}]
28:["$","h1",null,{"children":"Using language models to encrypt text as text"}]
29:["$","p",null,{"children":["With these two codecs in hand (text → bits → text and bits → text → bits) it was finally time to make my dream come true: ",["$","strong",null,{"children":"encrypting text as text"}],"."]}]
2a:["$","p",null,{"children":"At a high level, the pipeline is simply this:"}]
2b:["$","pre",null,{"children":["$","code",null,{"children":"encryption: text → bits → encrypted bits → encrypted text\ndecryption: encrypted text → encrypted bits → bits → text\n"}]}]
2c:["$","p",null,{"children":"And each arrow is perfectly computable and reversible."}]
2d:["$","h1",null,{"children":"Creating an encrypted chat protocol using the method"}]
2e:["$","p",null,{"children":"To demonstrate the idea in a more playful setting, I simulated an encrypted chat application in which encrypted messages resemble ordinary messages. Making this work requires solving a few practical problems."}]
2f:["$","h2",null,{"children":"Choosing a chat format"}]
30:["$","p",null,{"children":"First, we need a message format. I use the following:"}]
31:["$","pre",null,{"children":["$","code",null,{"children":"- A: message1\n- B: message2\n- A: message3\n...\n"}]}]
32:["$","p",null,{"children":["Each message is terminated by a ",["$","code",null,{"children":"\\n"}]," token, which is explicitly forbidden inside messages. This provides a clear stopping condition for the decoder. Without an end-of-message token, decoding would have no natural termination point and would continue indefinitely."]}]
33:["$","h2",null,{"children":"Making encrypted text finish gracefully"}]
34:["$","p",null,{"children":["It would be unfortunate if encrypted messages ended abruptly as soon as all bits were consumed. To avoid this, we treat ",["$","code",null,{"children":"\\n"}]," as an end-of-sequence token. While there are still bits left to encode, ",["$","code",null,{"children":"\\n"}]," is excluded from the token set. Once encoding is complete, ",["$","code",null,{"children":"\\n"}]," is reintroduced, and generation continues until it is sampled, producing a clean and natural-looking ending."]}]
35:["$","h2",null,{"children":"Choosing an encryption algorithm"}]
36:["$","p",null,{"children":"We also need an encryption algorithm that does not significantly inflate the message size. Ideally, the number of encrypted bits should be close to the number of original bits, since bit length is tied to entropy, and more bits often lead to longer sequences. A conversation with unusually high entropy might attract unwanted attention."}]
37:["$","p",null,{"children":"I decided to use AES in counter mode (AES-CTR). CTR mode behaves like a stream cipher: it XORs the plaintext with a pseudorandom keystream derived from AES, producing ciphertext that is indistinguishable from random under standard assumptions. Unlike authenticated modes, it introduces no padding or expansion beyond a fixed-size nonce. However, this nonce does not need to be secret; it only needs to be unique. I decided to use the number of previous messages as the nonce, avoiding the need to send it alongside the message. This ultimately reduces the number of bits that need to be encoded as text."}]
38:["$","h2",null,{"children":"Detecting whether a message is encrypted"}]
39:["$","p",null,{"children":["Finally, I needed a way to distinguish encrypted from plain messages. In my experimentation, decrypting a non-encrypted message has a high chance of failing the bits-to-text conversion because it often proposes tokens that would have been excluded from the ",["$","code",null,{"children":"top_k"}]," or ",["$","code",null,{"children":"top_p"}]," first tokens. I thus rely on this right now to determine if a message was encrypted or not. An alternative would be to prefix the encrypted message with a special token. During decoding, if the first token is this special token, the message is treated as unencrypted and displayed as-is."]}]
3a:["$","h1",null,{"children":"Demo time!"}]
3b:["$","p",null,{"children":"Actions:"}]
3c:["$","ul",null,{"children":["\n",["$","li",null,{"children":["You can switch between normal and encrypted mode with the ",["$","$L41",null,{}]," / ",["$","$L42",null,{"className":"material-symbols-outlined","component":"span","sx":{"transform":"translateY(6px)"},"children":"domino_mask"}]]}],"\n",["$","li",null,{"children":["Switch user with ",["$","$L43",null,{}]," buton. - Delete all messages with\n",["$","$L44",null,{}]," button."]}],"\n",["$","li",null,{"children":["Reset default conversation with ",["$","$L45",null,{}]," button"]}],"\n",["$","li",null,{"children":["Change the decrypt conversation with the ",["$","$L46",null,{}]," button - Send message by\npressing \"Enter\" or clicking on the ",["$","$L47",null,{}]]}],"\n"]}]
3d:["$","p",null,{"children":"The colored dot on each message indicates knowledge about encryption:"}]
3e:["$","ul",null,{"children":["\n",["$","li",null,{"children":[["$","$L48",null,{"color":"success","sx":{"fontSize":13}}],": Message is\nnot encrypted"]}],"\n",["$","li",null,{"children":[["$","$L48",null,{"color":"warning","sx":{"fontSize":13}}],": We don't know\nif the message is encrypted or not"]}],"\n",["$","li",null,{"children":[["$","$L48",null,{"color":"error","sx":{"fontSize":13}}],": Message is\nencrypted"]}],"\n"]}]
3f:["$","$L49",null,{}]
40:["$","$L17",null,{}]
f:[["$","meta","0",{"charSet":"utf-8"}],["$","meta","1",{"name":"viewport","content":"width=device-width, initial-scale=1"}]]
4a:I[27201,["/_next/static/chunks/ff1a16fafef87110.js","/_next/static/chunks/d2be314c3ece3fbe.js"],"IconMark"]
d:null
11:[["$","title","0",{"children":"Antoine's Blog"}],["$","meta","1",{"name":"description","content":"A blog where I share my thoughts about deep learning and my interests."}],["$","link","2",{"rel":"icon","href":"/favicon.ico?favicon.722d4cfd.ico","sizes":"257x256","type":"image/x-icon"}],["$","$L4a","3",{}]]
